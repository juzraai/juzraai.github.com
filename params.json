{"name":"juzraai's developer page","tagline":"/* burying myself into coding */","body":"# Hi,\r\nI'm *Zsolt Jur√°nyi* from Hungary, I store and share my open-source projects here. I'm programming in Java, this language became my favourite as we first met in university. I develop mostly crawler applications, see my publication database crawler **PubSearch** and my current project **CORDIS Projects Crawler**.\r\n\r\nCheck out my [repos](https://github.com/juzraai?tab=repositories), and also my [blog](http://juzraai-en.blogspot.com/)!\r\n\r\n## My projects\r\n\r\n### [CORDIS Projects Crawler](http://juzraai.github.io/Cordis-Projects-Crawler/) (currently developing)\r\nThis app/API can fetch information of all [CORDIS projects](http://cordis.europa.eu/projects/). There's a CLI for users, and there are handy methods for developers! :) It can download project pages and publication list JSON strings into an output directory, can skip already existing files, ... and see the [feature list](https://github.com/juzraai/Cordis-Projects-Crawler)! :D\r\n\r\n### [Crawljax Plugins](http://juzraai.github.io/Crawljax-Plugins/) (development paused)\r\nMy useful Crawljax Plugins. Currently there's only one: SaveHTML which helps you to save every DOM state to file. Filename and directory name will be created just as they are in the URL, so you can create a site mirror with Crawljax and my SaveHTML plugin.\r\n\r\nCheck out my URL2File class too, it can be useful in other cases too.\r\n\r\n### [HeritrixRemote](http://juzraai.github.io/HeritrixRemote/) (development paused)\r\nI'm currently working on this application which is needed at work. The aim is to simplify controlling Heritrix 3.x crawler from the command line especially when you've got a lot of crawling jobs to manage.\r\n\r\n### PubSearch (abandoned after 1.1)\r\nThis was my thesis at university. It crawls websites of publication databases, fetches publications of the given author and stores them in a database. I stopped development at version 1.1 because it's not flexible enough to crawl any kinds of sites, so I moved on to PubSearch 2.\r\n\r\n### PubSearch 2 (development paused)\r\nThe crawling engines are now outside of the application, in 3rd party libraries, and PubSearch 2 will communicate with them thru an interface. PubSearch 1.1 crawler engine will be transformed to use that interface and bundled with PubSearch 2, so old crawler definition files for PubSearch 1.1 will remain useable.\r\n\r\nThe project was planned, started, but my time and motivation flew away somehow :S ... but maybe I'll continue it someday.","google":"UA-41050675-1","note":"Don't delete this file! It's used internally to help with page regeneration."}